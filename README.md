# ArabicTokenzier
This tokenizer was trained using the arabic dataset of <a href="https://huggingface.co/datasets/wikimedia/wikipedia">Wikipedia</a> ,it's based on the wordpiece algorithm


## Citations
1] 	 <a href="https://doi.org/10.48550/arXiv.2012.15524">Fast WordPiece Tokenization [arXiv.2012.15524] </a> <br />
2] @ONLINE{wikidump,<br />
    author = "Wikimedia Foundation",<br />
    title  = "Wikimedia Downloads",<br />
    url    = "https://dumps.wikimedia.org"<br />
}
